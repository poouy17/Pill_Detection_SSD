{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Using_finetuned_model_to_detect_pill_text의 사본","provenance":[{"file_id":"1B0aP549o7QLUKLJ7qoC1WIaFmbreGwbk","timestamp":1604220116701}],"collapsed_sections":[],"authorship_tag":"ABX9TyMcE+nqGO4jbyggYrghpBWb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"JbSakNrq7JIu","executionInfo":{"status":"ok","timestamp":1605541063705,"user_tz":-540,"elapsed":24294,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}},"outputId":"4f47052f-a652-48fd-894d-919a2495e43c","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2MGjOtqU7POF","executionInfo":{"status":"ok","timestamp":1605541099942,"user_tz":-540,"elapsed":31734,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}}},"source":["import os\n","\n","os.makedirs('/content/object_detection_demo_flow/fine_tuned_model/', exist_ok=True)\n","os.makedirs('/content/object_detection_demo_flow/training/', exist_ok=True)\n","\n","!cp -r \"/content/gdrive/My Drive/Capstone/Model/20.11.01_sota/fine_tuned_model/\" \"/content/object_detection_demo_flow/\"\n","!cp -r \"/content/gdrive/My Drive/Capstone/Model/20.11.01_sota/training/\" \"/content/object_detection_demo_flow/\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"qaugcMxd7ahy","executionInfo":{"status":"ok","timestamp":1605541099943,"user_tz":-540,"elapsed":31536,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}}},"source":["%%capture\n","import re\n","import numpy as np\n","\n","output_directory = '/content/object_detection_demo_flow/fine_tuned_model'\n","model_dir = '/content/object_detection_demo_flow/training'\n","\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)\n","!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory={output_directory} \\\n","    --trained_checkpoint_prefix={last_model_path}"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyBjQCwp7f2U","executionInfo":{"status":"ok","timestamp":1605541099944,"user_tz":-540,"elapsed":31365,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}},"outputId":"8fc8b151-b540-4f7c-f3ee-1c4b69e02f45","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls {output_directory}"],"execution_count":4,"outputs":[{"output_type":"stream","text":["checkpoint\t\t\tmodel.ckpt.index  saved_model\n","frozen_inference_graph.pb\tmodel.ckpt.meta\n","model.ckpt.data-00000-of-00001\tpipeline.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jm_nNo6T7gHr","executionInfo":{"status":"ok","timestamp":1605541099944,"user_tz":-540,"elapsed":31048,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}},"outputId":"62f2bb17-181d-443d-bcc8-2e6d8a760e9b","colab":{"base_uri":"https://localhost:8080/"}},"source":["pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n","assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)\n","!ls -alh {pb_fname}"],"execution_count":5,"outputs":[{"output_type":"stream","text":["-rw------- 1 root root 19M Nov 16 15:37 /content/object_detection_demo_flow/fine_tuned_model/frozen_inference_graph.pb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZEJ5rTYg70L0","executionInfo":{"status":"ok","timestamp":1605541110783,"user_tz":-540,"elapsed":10835,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}}},"source":["os.makedirs('/content/object_detection_demo_flow/data/images/final_test', exist_ok=True)\n","!cp -r \"/content/gdrive/My Drive/Capstone/Sample/\" \"/content/object_detection_demo_flow/data/images/final_test\""],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"YhUdi2Io8dWj","executionInfo":{"status":"ok","timestamp":1605541120252,"user_tz":-540,"elapsed":20301,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}}},"source":["\n","os.makedirs('/content/object_detection_demo_flow/data/annotations/', exist_ok=True)\n","!cp -r \"/content/gdrive/My Drive/Capstone/Model/20.11.01_sota/annotations/\" \"/content/object_detection_demo_flow/data/annotations\"\n","\n","label_map_pbtxt_fname = '/content/object_detection_demo_flow/data/annotations/annotations/label_map.pbtxt'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"8dAEqQ6m7gSa","executionInfo":{"status":"ok","timestamp":1605541120253,"user_tz":-540,"elapsed":20285,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}},"outputId":"3e9c42a7-8b57-4d2d-b872-bf76a20c3dd8","colab":{"base_uri":"https://localhost:8080/"}},"source":["import glob\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT = pb_fname\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = label_map_pbtxt_fname\n","\n","# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n","PATH_TO_TEST_IMAGES_DIR = '/content/object_detection_demo_flow/data/images/final_test/Sample'\n","\n","assert os.path.isfile(pb_fname)\n","assert os.path.isfile(PATH_TO_LABELS)\n","TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n","assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n","print(TEST_IMAGE_PATHS)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["['/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_184116.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_174303.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_165613.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_165446.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_184102.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_183029.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_172307.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_165239.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_162234.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_184137.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_174421.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_171918.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_171831.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201004_184004.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_164730.jpg', '/content/object_detection_demo_flow/data/images/final_test/Sample/20201005_170143.jpg']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ItVH5G5DD_pY","executionInfo":{"status":"ok","timestamp":1605541131577,"user_tz":-540,"elapsed":6414,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}},"outputId":"7f4c8079-d676-41d3-a753-94ceb43d0d34","colab":{"base_uri":"https://localhost:8080/"}},"source":["\n","%tensorflow_version 1.x\n","!pip install tf_slim\n","#!pip install gast==0.2.2"],"execution_count":9,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","Collecting tf_slim\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl (352kB)\n","\u001b[K     |████████████████████████████████| 358kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from tf_slim) (0.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n","Installing collected packages: tf-slim\n","Successfully installed tf-slim-1.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VHLOqAE7DR-O","executionInfo":{"status":"ok","timestamp":1605541474275,"user_tz":-540,"elapsed":165569,"user":{"displayName":"조운택","photoUrl":"","userId":"13569714551873309051"}},"outputId":"ed13d27e-5e02-4f54-a555-c11a17db8b00","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Pc-j1CnxCOr-OpEcwRn4vCERs30jqaS_"}},"source":["%cd /content/gdrive/My Drive/Capstone/Model/models/research/object_detection\n","\n","\n","\n","import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","bounding_box_location = []\n","crop_images = []\n","\n","from collections import defaultdict\n","from io import StringIO\n","# This is needed to display the images.\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","from object_detection.utils import ops as utils_ops\n","\n","from object_detection.utils import label_map_util\n","\n","from object_detection.utils import visualization_utils as vis_util\n","\n","\n","def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())\n","\n","num_classes = get_num_classes(label_map_pbtxt_fname)\n","\n","print(num_classes)\n","\n","\n","detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(\n","    label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","\n","def load_image_into_numpy_array(image):\n","    (im_width, im_height) = image.size\n","    return np.array(image.getdata()).reshape(\n","        (im_height, im_width, 3)).astype(np.uint8)\n","\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (12, 8)\n","\n","\n","def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict\n","\n","\n","for image_path in TEST_IMAGE_PATHS:\n","    image = Image.open(image_path)\n","    print(image_path)\n","    # the array based representation of the image will be used later in order to prepare the\n","    # result image with boxes and labels on it.\n","    image_np = load_image_into_numpy_array(image)\n","    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","    image_np_expanded = np.expand_dims(image_np, axis=0)\n","    # Actual detection.\n","    output_dict = run_inference_for_single_image(image_np, detection_graph)\n","    # Visualization of the results of a detection.\n","    image_np, bounding_box_location = vis_util.visualize_boxes_and_labels_on_image_array(\n","        image_np,\n","        output_dict['detection_boxes'],\n","        output_dict['detection_classes'],\n","        output_dict['detection_scores'],\n","        category_index,\n","        instance_masks=output_dict.get('detection_masks'),\n","        use_normalized_coordinates=True,\n","        line_thickness=8)\n","    plt.figure(figsize=IMAGE_SIZE)\n","    plt.imshow(image_np)\n","    plt.show()\n","    plt.clf()\n","    plt.close('all')\n","    \n","    if(len(bounding_box_location) != 0):\n","      print(\"##################crop######################\")\n","      crop_img = image.crop((bounding_box_location[0], bounding_box_location[2], bounding_box_location[1], bounding_box_location[3]))\n","      crop_images.append(crop_img)\n","      plt.figure(figsize=IMAGE_SIZE)\n","      plt.imshow(crop_img)\n","      plt.show()\n","      del bounding_box_location[:]\n","      plt.clf()\n","      plt.close('all')"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"b2pVPoFP5xLr"},"source":[""],"execution_count":null,"outputs":[]}]}